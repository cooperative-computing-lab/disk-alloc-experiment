
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{framed}
\usepackage{floatrow}
\usepackage{comment}
\usepackage{hyperref}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Loop Device Performance Overhead for Constraining Batch Job Tasks:\\A Technical Report and Experiment in Metascience}


\author{\IEEEauthorblockN{Nathaniel Kremer-Herman}
\IEEEauthorblockA{Department of Computer Science and Engineering\\University of Notre Dame\\
Email: nkremerh@nd.edu}}


\maketitle


\begin{abstract}
Providing a resource-constrained sandbox for batch job tasks is a need which has been rising in importance within distributed computing. In many cases, the batch jobs being scheduled are sent to machines which the submitter does not own. Without resource usage guarantees, there is the potential that a task will consume more resources than desired. This may come at the cost of decreasing the availability of the host machine for future work. Or worse, the task may inadvertently take advantage of a flaw which can damage the host machine in an irreparable manner. Presented in this report is a method to keep a batch job task within a secure sandbox constrained by an arbitrary disk size with reasonable performance guarantees as compared to the task executing on the host machine's native filesystem.
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}
Currently, batch job task execution occurs in sandboxes which prevent a task from interfering with host processes. However, a sandbox namespace is not a strict enough mechanism to prevent storage exhaustion. A popular method of providing resource constraint is to run a sentinel process which periodically watches a task and alerts the batch system when that task has used more resources than it initially requested. However, this provides only an after-the-fact assessment of the task's resource usage. There is not much support in POSIX to instantaneously handle storage exhaustion, but Linux loop devices provide passive storage constraint of a sandbox. Loop device storage constraint is useful for both the owner of the host machine and the user running a batch job task. In a loop device sandbox, a task cannot progress once it has used the storage space available to the device. This prevents the problem of storage exhaustion from impacting other tasks since the task is in its own namespace. It also prevents a task from consuming enough disk space to make the host machine unavailable for future incoming work.

The primary motivation for using loop devices to constrain tasks is to ensure that each one has exactly as much space as it needs to complete while also preventing the task from consuming more space than it has been allotted. Without loop devices, a task using more storage than it requested may affect the availability of the host system for future work since it may use up all remaining storage on the host machine. By creating a loop device, the task establishes a sandbox that has an arbitrary size. Not only does this method guarantee disk space {\it{for}} the task, it also ensures a greater level of security for the host system {\it{from}} the batch job task. This added security is derived from a lower impact of failure due to storage exhaustion. 

\section{Loop Device Resource Sandbox}
A loop device is classified as a pseudo-device meaning it is represented as a normal file in the filesystem, but it can be mounted and treated as a physical device. This is the key factor which provides the resource constraint to the sandbox. Because a loop device is treated like any other physical device on the system, it has a maximum capacity. This is equivalent to the size of the file mounted as the loop device. Once a task running within the loop device reaches that maximum capacity, there are no other resources with which that task can run. This provides a hard limit to the disk space a task can consume.

\subsection{Runtime Expectations}
Once a loop device is mounted and prepared to handle I/O requests, a batch job task can now be placed within by setting its working directory to the path of the file which was created to serve as the mountpoint of the device. Once the task is within this newly-created sandbox, it is constrained to a maximum size equivalent to the size of the mounted file. Because of this, a task cannot use more disk space than was allocated, and any inappropriate access to disk space will not be permitted. This is further reinforced by the fact that the host operating system views a loop device like any other physical device and will treat its file size as if it were a storage capacity imposed by physical limitations.

The fact that a loop device is treated as a separate physical device provides a barrier between the batch job task's execution environment and the host machine's native filesystem. In the case of extreme failure, the task will not harm the native filesystem but instead will cause damage to the integrity of the loop device. This can be remedied by simply deallocating the loop device.

\section{Implementation}

\subsection{Allocation}
To allocate a loop device, one must first create a file filled with arbitrary data up to a specified size. This is accomplished by executing a command like {\tt{dd}} in Linux systems. Following the creation of this file, it is bound to an available loop device using the {\tt{losetup}} Linux comand. The operating system of our test platform has eight available loop devices by default with the option to allow up to 256. This allows for 256 resource constrained tasks to run concurrently. Following this step, a new filesystem is instantiated on the loop device. Finally, the loop device is mounted, after which it is ready for I/O.

\subsection{Deallocation}
The deallocation of a loop device is simpler than its setup. First, one must unmount the loop device via the invocation of the {\tt{umount}} command. Then the file used as the mountpoint is unbound from its associated loop device. The file is once again visible to the native filesystem and is removed.

\subsection{Library Implementation}
We have simplified the allocation and deallocation of loop devices with the creation of a new library consisting of two functions, {\tt{disk\_alloc\_create}} and {\tt{disk\_alloc\_delete}} which handle the creation and deletion of a loop device respectively. This library is implemented in the CCTools software suite within the Work Queue batch system and the Makeflow workflow management system.

In Work Queue, a batch job task first instantiates a loop device on the host machine as an execution sandbox. Once the device is set up, the task may begin its work and will continue to do so until it either completes successfully or runs out of space at which point it will fail. Regardless whether the task is successful or not, it will then clean up its workspace by deallocating the loop device and removing its working directory.

A task's resource requirements are specified within a makeflow. These constraints include the number of cores the task will utilize, the amount of memory it requires, and the amount of disk space it will consume. The disk allocation library is passed the disk space requirements from the makeflow via Work Queue. This disk space requirement is used as the size of the loop device.

\section{Evaluation}
In order to properly capture the relevant performance characteristics of loop devices as compared to the native filesystem, the evaluation of the library consists of standard micro-benchmarks which test throughput performance as well as overhead of loop device instantiation and deletion.

\subsection{Metrics}
We measured the performance of loop device allocations compared to a native filesystem with three micro-benchmarks: read, write, and file metadata throughput. Our test platform has the following specifications: 3GB RAM, 2 x Intel Core 2 Duo processors, 250GB SATA drive (16 heads, 7,200 RPM, 16,383 cylinders, 63 sectors/track, queue depth 32, max I/O data transfer rate 300 MB/s, max sustained data transfer rate 125 MB/s, internal data transfer rate 1,695 Mb/s). We measured sequential read and sequential write throughput of a file 95\% of its allocation size. Metadata throughput was measured by timing a loop device's creation of 10,000 blank files, retrieving file status on each file, then deleting each file. We tested these micro-benchmarks across three filesystems: ext2, ext3, and ext4 which are commonly available. 

Each micro-benchmark test ran for twenty iterations then created a nested loop device within the current device. This process was completed twenty times resulting in a loop device nested within twenty other loop devices. This nesting shows a reduction in device performance the deeper the device is nested. Though nesting a loop device within another is feasible in terms of performance, it is not the expected behavior of a workflow. Regardless, we demonstrate that an entire workflow can run within a storage container and still perform reasonably. The aim of these micro-benchmarks was to demonstrate that a loop device does not experience a steep performance decline in order to provide a resource guarantee for a workflow task. We have provided the first ten nest depths of the test as they are most pertinent.

We also measured the overhead associated with the creation and deletion of loop devices. Much like the throughput performance tests, each test ran for twenty iterations. However, we did not take into account nesting for these two micro-benchmarks. We measured the time to create a loop device sandbox starting at a size of 1MB and increasing in logarithmic scale to 8GB. We also timed how long it took to delete the loop device sandbox.

\subsection{Performance}
The results presented in Figures \ref{fig:2gb}, \ref{fig:4gb}, and \ref{fig:8gb} display the median value across each nest depth with the respective median absolute deviation. This format was chosen in order to present the most accurate representation of loop device performance. The performance of a loop device mounted with the any of the ext family filesystems performs comparably in many cases with the native filesystem. The native filesystem of our test platform is ext4. By using non-trivial sizes for the test allocations (in the order of GBs), our micro-benchmarks simulate sustained I/O expected of a batch job task completing a realistic work load. This is opposed to burst I/O which would be seen in tasks smaller than 1GB. 

Since the majority of workflows execute within a nest depth of zero, the most pertinent results are located within the first few nest depths. The rare case of a nested loop device task would occur if a batch job task executed another workflow locally. For added perspective, we have provided the performance results down to a nest level of ten. This would be indicative of the performance of a task recursively executing ten separate workflows locally. 
We examine the MB/s throughput of the device in the read and write micro-benchmarks at each nest depth (from the native filesystem to a loop device nest depth of ten).

We note an equivalent performance to the native filesystem with a nest depth of zero (a single loop device) in the read micro-benchmark. As nest depth increases, there is a linear decrease in read speed. We demonstrate that the resource constraint provided by loop devices does not come at the cost of performance for most cases. Should a user execute a workflow which would make use of nested loop devices, they must consider the nest depth at which they see diminishing returns. Reasonable, though not comparable, performance can be expected within the first four nest levels of loop devices after which read performance begins to become more lackluster compared to the native filesystem.

Write performance remains comparable to the native filesystem at a deeper nest level than read performance. However, there is a sharper degradation in performance once a nest depth of four is reached. Similar to the read performance, the user would have to consider the added cost of resource guarantees and constraint if their workflow will execute at higher nest depths. 

We note that the metadata micro-benchmark's performance is notably faster for the first four depth levels due to the asynchronous operation of loop devices. This behavior is enabled by default. Since metadata performance is measured by how quickly 10,000 empty files are instantiated, we note this asynchronous batching method is quicker for metadata operations. As is the case with the other two micro-benchmarks, the metadata performance of a loop device relative to the native filesystem drops off at higher nest levels. In our metadata testing, this drop off occurs at a nest depth of five. Like in the read and write results, the user would have to determine if the slower performance is worth the resource guarantees should their workflow execute at high nest depths.

The results presented in Figure \ref{fig:create} display the median value of each sandbox size's creation time. Like in the throughput performance results, we have provided the median absolute deviation for each sandbox size. We can see a proportional linear increase in creation time as the sandbox size increases which is expected. The smaller sandbox sizes (less than 1GB) take very little time to instantiate with 512MB taking approximately six seconds to set up. This small overhead should have minimal impact upon performance except in cases where the task would run in a matter of a few seconds where the overhead for instantiating a loop device could double the duration of a task. Considering larger allocations, we see that the overhead is still quite reasonable. 1GB sandboxes across all three ext filesystems show a creation overhead of approximately eleven seconds. On the high end of our testing spectrum, we see an 8GB allocation having a seventy second instantiation duration. In normal cases, we notice that allocations of 8GB or greater experience computation on the order of tens of minutes or longer, so the creation overhead does not seem to dominate a task's duration. In certain cases, however, only a small fraction of the data passed into the sandbox will be relevant to computation. In this case, the creation overhead may be as great as the time spent on computation. We note that file transfer for the data would dominate a task's duration in this case and not the loop device instantiation.

Loop device deletion seems to be almost trivial in duration. In our implementation, the sandbox is dereferenced once the task is complete. In the future, we may opt to overwrite the disk space consumed by the sandbox with zeros before dereferencing the pointers to the disk, however this was not a priority in the current implementation. In all instances of our deletion overhead testing, we can see that the time taken to remove a loop device is less than half a second. As is expected, the 8GB allocation took the longest to remove. In the lower end of our testing spectrum, we note that the smallest allocations took longer to remove than the mid-range allocation sizes in ext3 and ext4. We posit this may be due to handling within ext3 and ext4 which derefences very small files in a less efficient manner with a neglibgible performance decrease compared to a more advanced handling mechanism for larger files. The largest difference we observe is approximately 0.05 seconds between a 16MB and a 32MB allocation which is a slowdown no human user would likely notice.

\subsection{Impact of Failure}
Loop devices allow for a task to fail more cleanly than if it were not in a strict sandbox. The integrity of the host filesystem is not in as much danger of compromise. In the case of a failure, the loop device may become corrupted and unusable. In this case, the solution is to unmount and remove the loop device. This provides a benefit to the owner of the failed batch job task since the host machine will still be available for future work. Considering an incident without loop devices being used, it is possible that extreme failure can cause problems with the host filesystem precisely because the task is not isolated within its own storage constrained sandbox. All but the most catastrophic failures are contained within a loop device, and it exits cleanly.

\section{Reproducibility Considerations}
Although this experiment is primarily intended to demonstrate the viability of loop devices for storage constraint, we also make special considerations for the reproducibility of the experiment. In particular, we have already documented a comprehensive specification of the disk used in our test platform. To achieve greater reproducibility however, we must create a near-exact replica of the test platform. This can be done with a virtual machine which will be able to simulate not only the software of the test machine but also the hardware and kernel. The next step toward reproducibility of this work would be for us to host an image of our test platform freely available.

Availability of the experiment is an important consideration for reproducibility. We have provided the source code, which is freely available on GitHub at {\url{https://github.com/cooperative-computing-lab/disk-alloc-experiment}}. Within this git repository we have included the data collected for this technical report and a convenient Makefile which makes running the experiment quite simple. This Makefile downloads all the dependent code for the experiment except some software listed in the {\tt{README}} located in the repository as well as runs the specified tests, generates graphs from the test data, and compiles this paper. We chose to provide this depth of content because reproducibility and availability are high priorities with respect to this technical report.

Another consideration for reproducibility is the anticipated runtime of the tests. In particular, the performance tests (read, write, and file metadata tests) took approximately two weeks to complete on the test platform. This was due to the twenty iterations taken on each test which ran to a nest depth of twenty. This was a heavy I/O load for the test platform. However, the overhead tests (creation and deletion tests) took much less time to run at approximately two hours. The vast difference between the two comes from the lack of nest performance taken into account by the overhead tests.

\section{Conclusion}
Though resource constraint does not have much support, we present a novel method of constraining the storage usage of a batch job task through the use of loop devices. Loop devices, as opposed to a sentinel process, provide an instantaneous handling of storage exhaustion. This passive method of constraint keeps a task within its requested storage while also guaranteeing resources to it. This in turns prevents the task from using up enough space to affect the availability of the host machine for future work. We have shown that loop devices provide these benefits with a reasonable performance guarantee.

\newcommand{\cimage}[1]{$\vcenter{\hbox{\includegraphics[width=0.3\textwidth]{#1}}}$}

\begin{figure*}[t]
\begin{tabular}{ccc}
ext2 & \cimage{./previous_results/create_ext2.pdf} & \cimage{./previous_results/delete_ext2.pdf}\\
ext3 & \cimage{./previous_results/create_ext3.pdf} & \cimage{./previous_results/delete_ext3.pdf}\\
ext4 & \cimage{./previous_results/create_ext4.pdf} & \cimage{./previous_results/delete_ext4.pdf}\\
\end{tabular}
\caption{Loop Device Creation/Deletion Overhead}
\label{fig:create}
\end{figure*}

\newpage
\begin{figure*}[t]
\begin{tabular}{cccc}
ext2 & \cimage{./previous_results/read_ext2_2GB.pdf} & \cimage{./previous_results/write_ext2_2GB.pdf} & \cimage{./previous_results/meta_ext2_2GB.pdf}\\
ext3 & \cimage{./previous_results/read_ext3_2GB.pdf} & \cimage{./previous_results/write_ext3_2GB.pdf} & \cimage{./previous_results/meta_ext3_2GB.pdf}\\
ext4 & \cimage{./previous_results/read_ext4_2GB.pdf} & \cimage{./previous_results/write_ext4_2GB.pdf} & \cimage{./previous_results/meta_ext4_2GB.pdf}\\
\end{tabular}
\caption{Loop Device Performance with 2GB Allocations}
\label{fig:2gb}
\end{figure*}

\begin{figure*}[t]
\begin{tabular}{cccc}
ext2 & \cimage{./previous_results/read_ext2_4GB.pdf} & \cimage{./previous_results/write_ext2_4GB.pdf} & \cimage{./previous_results/meta_ext2_4GB.pdf}\\
ext3 & \cimage{./previous_results/read_ext3_4GB.pdf} & \cimage{./previous_results/write_ext3_4GB.pdf} & \cimage{./previous_results/meta_ext3_4GB.pdf}\\
ext4 & \cimage{./previous_results/read_ext4_4GB.pdf} & \cimage{./previous_results/write_ext4_4GB.pdf} & \cimage{./previous_results/meta_ext4_4GB.pdf}\\
\end{tabular}
\caption{Loop Device Performance with 4GB Allocations}
\label{fig:4gb}
\end{figure*}

\begin{figure*}[t]
\begin{tabular}{cccc}
ext2 & \cimage{./previous_results/read_ext2_8GB.pdf} & \cimage{./previous_results/write_ext2_8GB.pdf} & \cimage{./previous_results/meta_ext2_8GB.pdf}\\
ext3 & \cimage{./previous_results/read_ext3_8GB.pdf} & \cimage{./previous_results/write_ext3_8GB.pdf} & \cimage{./previous_results/meta_ext3_8GB.pdf}\\
ext4 & \cimage{./previous_results/read_ext4_8GB.pdf} & \cimage{./previous_results/write_ext4_8GB.pdf} & \cimage{./previous_results/meta_ext4_8GB.pdf}\\
\end{tabular}
\caption{Loop Device Performance with 8GB Allocations}
\label{fig:8gb}
\end{figure*}

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\end{document}


